<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Real-Time Speech-to-Text Transcription</title>
    <style>
      body {
        font-family: system-ui, -apple-system, sans-serif;
        max-width: 800px;
        margin: 2rem auto;
        padding: 0 1rem;
      }
      #output {
        margin-top: 1rem;
        padding: 1rem;
        border: 1px solid #ccc;
        border-radius: 4px;
        min-height: 200px;
        white-space: pre-wrap;
      }
      button {
        padding: 0.5rem 1rem;
        margin-right: 0.5rem;
        border: none;
        border-radius: 4px;
        background-color: #007bff;
        color: white;
        cursor: pointer;
      }
      button:disabled {
        background-color: #ccc;
        cursor: not-allowed;
      }
      .status {
        margin-top: 1rem;
        color: #666;
      }
    </style>
  </head>
  <body>
    <h1>Real-Time Speech-to-Text Transcription</h1>
    <button id="start-btn">Start Transcribing</button>
    <button id="stop-btn" disabled>Stop Transcribing</button>
    <div class="status" id="status">Status: Idle</div>
    <pre id="output"></pre>

    <script>
      let mediaRecorder;
      let audioContext;
      const outputElement = document.getElementById("output");
      const startButton = document.getElementById("start-btn");
      const stopButton = document.getElementById("stop-btn");
      const statusElement = document.getElementById("status");

      async function startTranscribing() {
        try {
          // Create audio context
          audioContext = new (window.AudioContext || window.webkitAudioContext)(
            {
              sampleRate: 16000,
            }
          );

          // Get media stream
          const stream = await navigator.mediaDevices.getUserMedia({
            audio: {
              channelCount: 1,
              sampleRate: 16000,
              echoCancellation: true,
              noiseSuppression: true,
            },
          });

          // Create MediaRecorder
          const options = {
            audioBitsPerSecond: 16000,
            mimeType: "audio/webm",
          };

          // Create audio worklet for processing
          const source = audioContext.createMediaStreamSource(stream);
          const processor = audioContext.createScriptProcessor(4096, 1, 1);
          let audioChunks = [];

          processor.onaudioprocess = (e) => {
            const inputData = e.inputBuffer.getChannelData(0);
            audioChunks.push(new Float32Array(inputData));

            // Send chunks every 3 seconds
            if (audioChunks.length >= 45) {
              // ~3 seconds of audio at 16kHz
              const audioData = concatenateAudioBuffers(audioChunks);
              sendAudioData(audioData);
              audioChunks = [];
            }
          };

          source.connect(processor);
          processor.connect(audioContext.destination);

          stopButton.disabled = false;
          startButton.disabled = true;
          statusElement.textContent = "Status: Recording...";
        } catch (error) {
          console.error("Error starting transcription:", error);
          statusElement.textContent = `Status: Error - ${error.message}`;
        }
      }

      function concatenateAudioBuffers(buffers) {
        let totalLength = 0;
        buffers.forEach((buffer) => (totalLength += buffer.length));

        const result = new Float32Array(totalLength);
        let offset = 0;

        buffers.forEach((buffer) => {
          result.set(buffer, offset);
          offset += buffer.length;
        });

        return result;
      }

      function float32ToInt16(float32Arr) {
        const int16Arr = new Int16Array(float32Arr.length);
        for (let i = 0; i < float32Arr.length; i++) {
          const s = Math.max(-1, Math.min(1, float32Arr[i]));
          int16Arr[i] = s < 0 ? s * 0x8000 : s * 0x7fff;
        }
        return int16Arr;
      }

      function createWavHeader(length) {
        const buffer = new ArrayBuffer(44);
        const view = new DataView(buffer);

        // "RIFF" chunk descriptor
        writeString(view, 0, "RIFF");
        view.setUint32(4, 36 + length * 2, true);
        writeString(view, 8, "WAVE");

        // "fmt " sub-chunk
        writeString(view, 12, "fmt ");
        view.setUint32(16, 16, true);
        view.setUint16(20, 1, true);
        view.setUint16(22, 1, true);
        view.setUint32(24, 16000, true);
        view.setUint32(28, 16000 * 2, true);
        view.setUint16(32, 2, true);
        view.setUint16(34, 16, true);

        // "data" sub-chunk
        writeString(view, 36, "data");
        view.setUint32(40, length * 2, true);

        return buffer;
      }

      function writeString(view, offset, string) {
        for (let i = 0; i < string.length; i++) {
          view.setUint8(offset + i, string.charCodeAt(i));
        }
      }

      async function sendAudioData(float32Audio) {
        try {
          statusElement.textContent = "Status: Processing audio...";

          // Convert to 16-bit PCM
          const int16Audio = float32ToInt16(float32Audio);

          // Create WAV header
          const wavHeader = createWavHeader(int16Audio.length);

          // Combine header and audio data
          const wavBlob = new Blob([wavHeader, int16Audio], {
            type: "audio/wav",
          });
          const arrayBuffer = await wavBlob.arrayBuffer();

          const response = await fetch(
            "https://api-inference.huggingface.co/models/openai/whisper-tiny",
            {
              method: "POST",
              headers: {
                Authorization: `Bearer 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`,
                "Content-Type": "audio/wav",
              },
              body: arrayBuffer,
            }
          );

          if (!response.ok) {
            const errorText = await response.text();
            throw new Error(
              `API request failed: ${response.status} - ${errorText}`
            );
          }

          const data = await response.json();

          if (data.text && data.text.trim()) {
            outputElement.textContent += data.text + " ";
            statusElement.textContent = "Status: Recording...";
          }
        } catch (error) {
          console.error("Error sending audio chunk:", error);
          statusElement.textContent = `Status: Error - ${error.message}`;
        }
      }

      function stopTranscribing() {
        if (audioContext) {
          audioContext.close();
        }
        stopButton.disabled = true;
        startButton.disabled = false;
        statusElement.textContent = "Status: Idle";
      }

      startButton.onclick = () => startTranscribing();
      stopButton.onclick = () => stopTranscribing();
    </script>
  </body>
</html>
